Ensemble methods
Ensemble methods, which combines several decision trees to produce better predictive performance than utilizing a single decision tree. The main principle behind the ensemble model is that a group of weak learners come together to form a strong learner.

Objective of Analysis
Minimization of risk and maximization of profit on behalf of the bank. To minimize loss from the bank’s perspective, the bank needs a decision rule regarding who to give approval of the loan and who not to. An applicant’s demographic and socio-economic profiles are considered by loan managers before a decision is taken regarding his/her loan application.

The German Credit Data contains data on 20 variables and the classification whether an applicant is considered a Good or a Bad credit risk for 1000 loan applicants. Here is a link to the German Credit data.  A predictive model developed on this data is expected to provide a bank manager guidance for making a decision whether to approve a loan to a prospective applicant based on his/her profiles.
EDA


Cost-Profit Consideration
Ultimately these statistical decisions must be translated into profit consideration for the bank. Let us assume that a correct decision of the bank would result in 35% profit at the end of 5 years. A correct decision here means that the bank predicts an application to be good or credit-worthy and it actually turns out to be credit worthy. When the opposite is true, i.e. bank predicts the application to be good but it turns out to be bad credit, then the loss is 100%. If the bank predicts an application to be non-creditworthy, then loan facility is not extended to that applicant and bank does not incur any loss (opportunity loss is not considered here). The cost matrix, therefore, is as follows:

Logistic regression model performance:

Bagging 
Bagging is used when the goal is to reduce the variance of a decision tree classifier. Here the objective is to create several subsets of data from training sample chosen randomly with replacement. Now, each collection of subset data is used to train their decision trees. As a result, we end up with an ensemble of different models. Average of all the predictions from different trees are used which is more robust than a single decision tree.

Bagging Steps: 
•	Suppose there are N observations and M features in training data set. First, a sample from training data set is taken randomly with replacement.
•	A subset of M features are selected randomly and whichever feature gives the best split is used to split the node iteratively. 
•	The tree is grown to the largest. 
•	Above steps are repeated and prediction is given based on the aggregation of predictions from n number of trees.

Advantages:
•	Handles higher dimensionality data very well.
•	Handles missing values and maintains accuracy for missing data.

Disadvantages:
•	Since final prediction is based on the mean predictions from subset trees, it won’t give precise values for the regression model.
Boosting 
Boosting is another ensemble technique to create a collection of predictors. In this technique, learners are learned sequentially with early learners fitting simple models to the data and then analyzing data for errors. In other words, we fit consecutive trees (random sample) and at every step, the goal is to solve for net error from the prior tree. When an input is misclassified by a hypothesis, its weight is increased so that next hypothesis is more likely to classify it correctly. By combining the whole set at the end converts weak learners into better performing model.

Boosting Steps:
•	Draw a random subset of training samples d1 without replacement from the training set D to train a weak learner C1
•	Draw second random training subset d2 without replacement from the training set and add 50 percent of the samples that were previously falsely classified/misclassified to train a weak learner C2
•	Find the training samples d3 in the training set D on which C1 and C2 disagree to train a third weak learner C3
•	Combine all the weak learners via majority voting.

Advantages:
•	Supports different loss function.
•	Works well with interactions.

Disadvantages:
•	Prone to over-fitting.
•	Requires careful tuning of different hyper-parameters

